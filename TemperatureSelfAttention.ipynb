{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TemperatureSelfAttention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxnkHNfRXc8i9YPEA479qz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLRG-CEFET-RJ/attention-mechanism/blob/main/TemperatureSelfAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K6mVmNS2IIu"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, zipfile, io\n",
        "from numpy import array\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import gc\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNIceLOZw8Lt"
      },
      "source": [
        "Baixar o arquivo com o conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d51q0jEW3FzC"
      },
      "source": [
        "zip_file_url = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip'\n",
        "\n",
        "r = requests.get(zip_file_url, stream=True)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBT49NJOxIaA"
      },
      "source": [
        "Carregar o arquivo (em formato CSV) para um objeto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_AA4hCM3eG9"
      },
      "source": [
        "df = pd.read_csv('jena_climate_2009_2016.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXwjsIVBxWXJ"
      },
      "source": [
        "Visualização dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "_rUGPc4E3hKW",
        "outputId": "07116d1c-cbc8-4813-8445-d85e58905b53"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date Time</th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01.01.2009 00:10:00</td>\n",
              "      <td>996.52</td>\n",
              "      <td>-8.02</td>\n",
              "      <td>265.40</td>\n",
              "      <td>-8.90</td>\n",
              "      <td>93.3</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.94</td>\n",
              "      <td>3.12</td>\n",
              "      <td>1307.75</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.75</td>\n",
              "      <td>152.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01.01.2009 00:20:00</td>\n",
              "      <td>996.57</td>\n",
              "      <td>-8.41</td>\n",
              "      <td>265.01</td>\n",
              "      <td>-9.28</td>\n",
              "      <td>93.4</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.03</td>\n",
              "      <td>1309.80</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.50</td>\n",
              "      <td>136.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01.01.2009 00:30:00</td>\n",
              "      <td>996.53</td>\n",
              "      <td>-8.51</td>\n",
              "      <td>264.91</td>\n",
              "      <td>-9.31</td>\n",
              "      <td>93.9</td>\n",
              "      <td>3.21</td>\n",
              "      <td>3.01</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.02</td>\n",
              "      <td>1310.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.63</td>\n",
              "      <td>171.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01.01.2009 00:40:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.31</td>\n",
              "      <td>265.12</td>\n",
              "      <td>-9.07</td>\n",
              "      <td>94.2</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1309.19</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.50</td>\n",
              "      <td>198.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01.01.2009 00:50:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.27</td>\n",
              "      <td>265.15</td>\n",
              "      <td>-9.04</td>\n",
              "      <td>94.1</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.09</td>\n",
              "      <td>1309.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.63</td>\n",
              "      <td>214.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Date Time  p (mbar)  T (degC)  ...  wv (m/s)  max. wv (m/s)  wd (deg)\n",
              "0  01.01.2009 00:10:00    996.52     -8.02  ...      1.03           1.75     152.3\n",
              "1  01.01.2009 00:20:00    996.57     -8.41  ...      0.72           1.50     136.1\n",
              "2  01.01.2009 00:30:00    996.53     -8.51  ...      0.19           0.63     171.6\n",
              "3  01.01.2009 00:40:00    996.51     -8.31  ...      0.34           0.50     198.0\n",
              "4  01.01.2009 00:50:00    996.51     -8.27  ...      0.32           0.63     214.3\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxhP-fKGmwTQ"
      },
      "source": [
        "Preparação dos dados (converter a frequência de observações de 10 min para 1 hora)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2-UBKN04nfB",
        "outputId": "01bfb934-bd47-486e-a7c5-140a41f21854"
      },
      "source": [
        "print(df.shape)\n",
        "\n",
        "# slice [start:stop:step], starting from index 5, take every 6th record.\n",
        "df = df[5::6]\n",
        "\n",
        "print(df.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(420551, 15)\n",
            "(70091, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCLBLryn5jO9",
        "outputId": "933b75e8-e08e-4421-9323-3c596eab3b82"
      },
      "source": [
        "date_time = pd.to_datetime(df['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
        "date_time"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5        2009-01-01 01:00:00\n",
              "11       2009-01-01 02:00:00\n",
              "17       2009-01-01 03:00:00\n",
              "23       2009-01-01 04:00:00\n",
              "29       2009-01-01 05:00:00\n",
              "                 ...        \n",
              "420521   2016-12-31 19:10:00\n",
              "420527   2016-12-31 20:10:00\n",
              "420533   2016-12-31 21:10:00\n",
              "420539   2016-12-31 22:10:00\n",
              "420545   2016-12-31 23:10:00\n",
              "Name: Date Time, Length: 70091, dtype: datetime64[ns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rskIulhJyp23"
      },
      "source": [
        "Sumário estatístico da temperatura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufK0R6hF5yXO",
        "outputId": "587f84a1-39e2-4481-a308-06e676d2aa39"
      },
      "source": [
        "temperature = df['T (degC)']\n",
        "temperature.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    70091.000000\n",
              "mean         9.450482\n",
              "std          8.423384\n",
              "min        -22.760000\n",
              "25%          3.350000\n",
              "50%          9.410000\n",
              "75%         15.480000\n",
              "max         37.280000\n",
              "Name: T (degC), dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyM8VVrT4P7h"
      },
      "source": [
        "Iremos usar 1200 exemplos para treinar o modelo e 500 exemplos, tanto para validação quanto para teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caz6XhVK6TS2",
        "outputId": "3c893621-4811-4b99-ec80-0eb460f22b42"
      },
      "source": [
        "train_set = temperature[:1200]\n",
        "valid_set = temperature[69000:69500]\n",
        "test_set = temperature[69500:]\n",
        "print('Proporção de exemplos para treino: {:.2f}%'.format(len(train_set)/len(temperature)))\n",
        "print('Proporção de exemplos para validação: {:.2f}%'.format(len(valid_set)/len(temperature)))\n",
        "print('Proporção de exemplos para teste: {:.2f}%'.format(len(valid_set)/len(temperature)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proporção de exemplos para treino: 0.02%\n",
            "Proporção de exemplos para validação: 0.01%\n",
            "Proporção de exemplos para teste: 0.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQRYhhZ453sk"
      },
      "source": [
        "Janelas deslizantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utTP5osP6j-u"
      },
      "source": [
        "def window_generator(sequence, n_steps):\n",
        "    x, y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        \n",
        "        end_ix = i + n_steps\n",
        "        \n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        x.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return array(x), array(y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za8BkrsW60Zk"
      },
      "source": [
        "n_steps = 3\n",
        "train_x, train_y = window_generator(train_set.values, n_steps)\n",
        "valid_x, valid_y = window_generator(valid_set.values, n_steps)\n",
        "test_x, test_y = window_generator(test_set.values, n_steps)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR_p5reo69Ak"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, k, heads):\n",
        "    super().__init__()\n",
        "    self.k, self.heads = k, heads\n",
        "\n",
        "    self.tokeys    = nn.Linear(k, k * heads, bias=False)\n",
        "    self.toqueries = nn.Linear(k, k * heads, bias=False)\n",
        "    self.tovalues  = nn.Linear(k, k * heads, bias=False)   \n",
        "    self.unifyheads = nn.Linear(heads * k, k)\n",
        "\n",
        "  def forward(self, x):\n",
        "      b, t, k = x.size()\n",
        "      h = self.heads\n",
        "\n",
        "      queries = self.toqueries(x).view(b, t, h, k)\n",
        "      keys    = self.tokeys(x)   .view(b, t, h, k)\n",
        "      values  = self.tovalues(x) .view(b, t, h, k)\n",
        "\n",
        "\n",
        "      keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "      queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "      values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "\n",
        "\n",
        "      queries = queries / (k ** (1/2))\n",
        "      keys    = keys / (k ** (1/2))\n",
        "\n",
        "      # - get dot product of queries and keys, and scale\n",
        "      dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "      # - dot has size (b*h, t, t) containing raw weights\n",
        "\n",
        "      dot = F.softmax(dot, dim=2) \n",
        "      # - dot now contains row-wise normalized weights\n",
        "\n",
        "      out = torch.bmm(dot, values).view(b, h, t, k)\n",
        "\n",
        "      out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
        "      return self.unifyheads(out)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhso9L1pyjYq"
      },
      "source": [
        "results = {}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chw_7ZT7Ae0K"
      },
      "source": [
        "seed = 15\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic=True"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr9-ESd98EIf"
      },
      "source": [
        "n_epochs = 30\n",
        "batch_size = 32"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1jO19kg7HLl"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self,feature,target):\n",
        "        self.feature = feature\n",
        "        self.target = target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.feature)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        item = self.feature[idx]\n",
        "        label = self.target[idx]\n",
        "        item = torch.as_tensor(item, dtype = torch.float32)\n",
        "        label = torch.as_tensor(label, dtype = torch.float32)\n",
        "        return item, label\n",
        "\n",
        "train_ds = Dataset(train_x.reshape(train_x.shape[0],train_x.shape[1],1), train_y)\n",
        "valid_ds = Dataset(valid_x.reshape(valid_x.shape[0],valid_x.shape[1],1), valid_y)\n",
        "test_ds = Dataset(test_x.reshape(test_x.shape[0],test_x.shape[1],1), test_y)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size = 1, shuffle = False)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size = 1, shuffle = False)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, batch_size = 1, shuffle = False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-wLpTOv7ykO",
        "outputId": "e0102506-1945-4ee2-fbfd-cc9ab37d3675"
      },
      "source": [
        "dataset = Dataset(train_x.reshape(train_x.shape[0],train_x.shape[1],1),train_y)\n",
        "print(dataset.__getitem__(0))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[-8.0500],\n",
            "        [-8.8800],\n",
            "        [-8.8100]]), tensor(-9.0500))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C4KWf6W8CgA"
      },
      "source": [
        "def train_model(model, batch_size, n_epochs):\n",
        "    \n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "        for batch, (data, target) in enumerate(train_loader, 1):\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        model.eval() # prep model for evaluation\n",
        "        for data, target in valid_loader:\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(n_epochs))\n",
        "        \n",
        "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'valid_loss: {valid_loss:.5f}')\n",
        "        \n",
        "        print(print_msg)\n",
        "        \n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "  \n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZg2co41BGEh"
      },
      "source": [
        "def predict (model):\n",
        "  model.eval()  # prepare model for evaluation\n",
        "  sum = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = model(data)\n",
        "          loss = criterion(output,target)\n",
        "          sum += loss.item()\n",
        "\n",
        "\n",
        "  return (sum/len(test_loader))\n",
        " "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOElw0K2RFeA"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnKecXDvIwGY"
      },
      "source": [
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            SelfAttention(1, heads=2),\n",
        "            nn.Conv1d(3, 64, kernel_size = 1)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.fc1 = nn.Linear(64,50)\n",
        "        self.fc2 = nn.Linear(50,1)      \n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.view(-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model1 = Net1().to(device)\n",
        "#optimizer = torch.optim.Adam(model1.parameters(), lr=1e-5)\n",
        "#criterion = nn.MSELoss()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9d888Ru8HqT"
      },
      "source": [
        "#model1, train_loss, valid_loss = train_model(model1, batch_size, n_epochs)\n",
        "#results[\"Test \" + str(model1) + \"MSELoss: \"] = predict(model1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcHanaFZKRQf"
      },
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            SelfAttention(1, heads=2),\n",
        "            nn.Conv1d(3, 64, kernel_size = 1),\n",
        "            SelfAttention(1, heads=2)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.fc1 = nn.Linear(64,50)\n",
        "        self.fc2 = nn.Linear(50,1)     \n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.view(-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model2 = Net2().to(device)\n",
        "#optimizer = torch.optim.Adam(model2.parameters(), lr=1e-5)\n",
        "#criterion = nn.MSELoss()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHPV2GAr-VDr"
      },
      "source": [
        "#model2, train_loss, valid_loss = train_model(model2, batch_size, n_epochs)\n",
        "#results[\"Test \" + str(model2) + \"MSELoss: \"] = predict(model2)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvifErQcKSEI"
      },
      "source": [
        "class Net3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net3,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(3, 64, kernel_size = 1),\n",
        "            SelfAttention(1, heads=2)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.fc1 = nn.Linear(64,50)\n",
        "        self.fc2 = nn.Linear(50,1)     \n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.view(-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model3 = Net3().to(device)\n",
        "#optimizer = torch.optim.Adam(model3.parameters(), lr=1e-5)\n",
        "#criterion = nn.MSELoss()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZZzEOl--ZVE"
      },
      "source": [
        "#model3, train_loss, valid_loss = train_model(model3, batch_size, n_epochs)\n",
        "#results[\"Test \" + str(model3) + \"MSELoss: \"] = predict(model3)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdzUiD59KS18"
      },
      "source": [
        "class Net4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net4,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            SelfAttention(1, heads=2),\n",
        "            nn.LayerNorm(1),\n",
        "            SelfAttention(1, heads=2),\n",
        "            nn.LayerNorm(1)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.fc1 = nn.Linear(3,50)\n",
        "        self.fc2 = nn.Linear(50,1)      \n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        #print(x.size())\n",
        "        x = x.view(-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model4 = Net4().to(device)\n",
        "#optimizer = torch.optim.Adam(model4.parameters(), lr=1e-5)\n",
        "#criterion = nn.MSELoss()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs6uZnPIAZOj"
      },
      "source": [
        "#model4, train_loss, valid_loss = train_model(model4, batch_size, n_epochs)\n",
        "#results[\"Test \" + str(model4) + \"MSELoss: \"] = predict(model4)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LI87g3GKTi-"
      },
      "source": [
        "class Net5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net5,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(3, 64, kernel_size = 1),\n",
        "            SelfAttention(1, heads=2),\n",
        "            nn.LayerNorm(1),\n",
        "            SelfAttention(1, heads=2),\n",
        "            nn.LayerNorm(1)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.fc1 = nn.Linear(64,50)\n",
        "        self.fc2 = nn.Linear(50,1)     \n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.view(-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model5 = Net5().to(device)\n",
        "#optimizer = torch.optim.Adam(model5.parameters(), lr=1e-5)\n",
        "#criterion = nn.MSELoss()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90MjYM7RAeG2"
      },
      "source": [
        "#model5, train_loss, valid_loss = train_model(model5, batch_size, n_epochs)\n",
        "#results[\"Test \" + str(model5) + \"MSELoss: \"] = predict(model5)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeMy-VHYKUUw"
      },
      "source": [
        "class Net6(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net6,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(3, 64, kernel_size = 1),\n",
        "            SelfAttention(1, heads=2),\n",
        "            SelfAttention(1, heads=2)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.fc1 = nn.Linear(64,50)\n",
        "        self.fc2 = nn.Linear(50,1)     \n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.view(-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model6 = Net6().to(device)\n",
        "#optimizer = torch.optim.Adam(model6.parameters(), lr=1e-5)\n",
        "#criterion = nn.MSELoss()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWKtFTFiAhAL"
      },
      "source": [
        "#model6, train_loss, valid_loss = train_model(model6, batch_size, n_epochs)\n",
        "#results[\"Test \" + str(model6) + \"MSELoss: \"] = predict(model6)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQERqVuyKVHB"
      },
      "source": [
        "class Net7(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net7,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            SelfAttention(1, heads=2),\n",
        "            nn.LayerNorm(1),\n",
        "            nn.Conv1d(3, 64, kernel_size = 1),\n",
        "            SelfAttention(1, heads=2),\n",
        "            nn.LayerNorm(1)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.fc1 = nn.Linear(64,50)\n",
        "        self.fc2 = nn.Linear(50,1)      \n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.view(-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model7 = Net7().to(device)\n",
        "#optimizer = torch.optim.Adam(model7.parameters(), lr=1e-5)\n",
        "#criterion = nn.MSELoss()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFgSwML9AlW7"
      },
      "source": [
        "#model7, train_loss, valid_loss = train_model(model7, batch_size, n_epochs)\n",
        "#results[\"Test \" + str(model7) + \"MSELoss: \"] = predict(model7)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYALqOfCn8Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb7df58-534c-4fc8-9179-8c89dac4d78a"
      },
      "source": [
        "nets = [Net1(), Net2(), Net3(), Net4(), Net5(), Net6(), Net7()]\n",
        "\n",
        "for n in nets:\n",
        "  model = n.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "  print(model.modules)\n",
        "  model, train_loss, valid_loss = train_model(model, batch_size, n_epochs)\n",
        "  results[\"Test \" + str(n) + \"MSELoss: \"] = predict(model)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.modules of Net1(\n",
            "  (layer1): Sequential(\n",
            "    (0): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")>\n",
            "[ 1/30] train_loss: 31.12170 valid_loss: 41.62337\n",
            "[ 2/30] train_loss: 30.66393 valid_loss: 41.85033\n",
            "[ 3/30] train_loss: 30.10756 valid_loss: 42.04209\n",
            "[ 4/30] train_loss: 29.40699 valid_loss: 42.14557\n",
            "[ 5/30] train_loss: 28.57475 valid_loss: 42.12058\n",
            "[ 6/30] train_loss: 27.59555 valid_loss: 41.92150\n",
            "[ 7/30] train_loss: 26.46112 valid_loss: 41.53049\n",
            "[ 8/30] train_loss: 25.17387 valid_loss: 40.93110\n",
            "[ 9/30] train_loss: 23.73233 valid_loss: 40.10346\n",
            "[10/30] train_loss: 22.14084 valid_loss: 39.05220\n",
            "[11/30] train_loss: 20.40900 valid_loss: 37.78384\n",
            "[12/30] train_loss: 18.55794 valid_loss: 36.28725\n",
            "[13/30] train_loss: 16.63014 valid_loss: 34.59451\n",
            "[14/30] train_loss: 14.68566 valid_loss: 32.81500\n",
            "[15/30] train_loss: 12.77013 valid_loss: 30.96900\n",
            "[16/30] train_loss: 10.93215 valid_loss: 29.08394\n",
            "[17/30] train_loss: 9.21842 valid_loss: 27.21188\n",
            "[18/30] train_loss: 7.67335 valid_loss: 25.34808\n",
            "[19/30] train_loss: 6.32754 valid_loss: 23.44706\n",
            "[20/30] train_loss: 5.19308 valid_loss: 21.61463\n",
            "[21/30] train_loss: 4.27595 valid_loss: 19.89979\n",
            "[22/30] train_loss: 3.56338 valid_loss: 18.27999\n",
            "[23/30] train_loss: 3.03396 valid_loss: 16.78402\n",
            "[24/30] train_loss: 2.65953 valid_loss: 15.42373\n",
            "[25/30] train_loss: 2.40415 valid_loss: 14.20660\n",
            "[26/30] train_loss: 2.23322 valid_loss: 13.13363\n",
            "[27/30] train_loss: 2.11941 valid_loss: 12.19970\n",
            "[28/30] train_loss: 2.04335 valid_loss: 11.38973\n",
            "[29/30] train_loss: 1.99217 valid_loss: 10.68804\n",
            "[30/30] train_loss: 1.95685 valid_loss: 10.07851\n",
            "<bound method Module.modules of Net2(\n",
            "  (layer1): Sequential(\n",
            "    (0): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (2): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")>\n",
            "[ 1/30] train_loss: 30.93894 valid_loss: 41.54821\n",
            "[ 2/30] train_loss: 30.83967 valid_loss: 41.67369\n",
            "[ 3/30] train_loss: 30.72225 valid_loss: 41.80292\n",
            "[ 4/30] train_loss: 30.59679 valid_loss: 41.93697\n",
            "[ 5/30] train_loss: 30.45979 valid_loss: 42.07073\n",
            "[ 6/30] train_loss: 30.30815 valid_loss: 42.20598\n",
            "[ 7/30] train_loss: 30.13518 valid_loss: 42.34485\n",
            "[ 8/30] train_loss: 29.93290 valid_loss: 42.48794\n",
            "[ 9/30] train_loss: 29.69632 valid_loss: 42.63293\n",
            "[10/30] train_loss: 29.41987 valid_loss: 42.77967\n",
            "[11/30] train_loss: 29.09285 valid_loss: 42.92842\n",
            "[12/30] train_loss: 28.70478 valid_loss: 43.07736\n",
            "[13/30] train_loss: 28.24104 valid_loss: 43.22478\n",
            "[14/30] train_loss: 27.68628 valid_loss: 43.36728\n",
            "[15/30] train_loss: 27.02206 valid_loss: 43.50348\n",
            "[16/30] train_loss: 26.23280 valid_loss: 43.62922\n",
            "[17/30] train_loss: 25.30129 valid_loss: 43.73435\n",
            "[18/30] train_loss: 24.21850 valid_loss: 43.80796\n",
            "[19/30] train_loss: 22.97380 valid_loss: 43.84381\n",
            "[20/30] train_loss: 21.54806 valid_loss: 43.83160\n",
            "[21/30] train_loss: 19.95301 valid_loss: 43.77209\n",
            "[22/30] train_loss: 18.19991 valid_loss: 43.65034\n",
            "[23/30] train_loss: 16.30893 valid_loss: 43.46936\n",
            "[24/30] train_loss: 14.32328 valid_loss: 43.23193\n",
            "[25/30] train_loss: 12.29955 valid_loss: 42.95146\n",
            "[26/30] train_loss: 10.35080 valid_loss: 42.64360\n",
            "[27/30] train_loss: 8.64739 valid_loss: 42.34276\n",
            "[28/30] train_loss: 7.39182 valid_loss: 42.08015\n",
            "[29/30] train_loss: 6.70294 valid_loss: 41.88188\n",
            "[30/30] train_loss: 6.42838 valid_loss: 41.72997\n",
            "<bound method Module.modules of Net3(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (1): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")>\n",
            "[ 1/30] train_loss: 32.09499 valid_loss: 40.10876\n",
            "[ 2/30] train_loss: 31.87803 valid_loss: 40.12476\n",
            "[ 3/30] train_loss: 31.51305 valid_loss: 40.08074\n",
            "[ 4/30] train_loss: 30.97882 valid_loss: 39.98214\n",
            "[ 5/30] train_loss: 30.20579 valid_loss: 39.84261\n",
            "[ 6/30] train_loss: 29.20694 valid_loss: 39.64683\n",
            "[ 7/30] train_loss: 27.95106 valid_loss: 39.37927\n",
            "[ 8/30] train_loss: 26.42610 valid_loss: 39.01388\n",
            "[ 9/30] train_loss: 24.69400 valid_loss: 38.52272\n",
            "[10/30] train_loss: 22.77812 valid_loss: 37.86171\n",
            "[11/30] train_loss: 20.61515 valid_loss: 36.99462\n",
            "[12/30] train_loss: 18.28513 valid_loss: 35.89648\n",
            "[13/30] train_loss: 15.82638 valid_loss: 34.57990\n",
            "[14/30] train_loss: 13.33976 valid_loss: 33.00377\n",
            "[15/30] train_loss: 11.03419 valid_loss: 31.18021\n",
            "[16/30] train_loss: 9.14560 valid_loss: 29.18648\n",
            "[17/30] train_loss: 7.90369 valid_loss: 27.11235\n",
            "[18/30] train_loss: 7.28852 valid_loss: 25.13513\n",
            "[19/30] train_loss: 6.95525 valid_loss: 23.20328\n",
            "[20/30] train_loss: 6.66352 valid_loss: 21.23166\n",
            "[21/30] train_loss: 6.37008 valid_loss: 19.17987\n",
            "[22/30] train_loss: 6.07788 valid_loss: 17.05506\n",
            "[23/30] train_loss: 5.78654 valid_loss: 14.91187\n",
            "[24/30] train_loss: 5.49550 valid_loss: 12.78887\n",
            "[25/30] train_loss: 5.20471 valid_loss: 10.70557\n",
            "[26/30] train_loss: 4.91538 valid_loss: 8.71781\n",
            "[27/30] train_loss: 4.63080 valid_loss: 6.91714\n",
            "[28/30] train_loss: 4.35436 valid_loss: 5.39001\n",
            "[29/30] train_loss: 4.08981 valid_loss: 4.22231\n",
            "[30/30] train_loss: 3.84307 valid_loss: 3.47571\n",
            "<bound method Module.modules of Net4(\n",
            "  (layer1): Sequential(\n",
            "    (0): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=3, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")>\n",
            "[ 1/30] train_loss: 31.03446 valid_loss: 41.54408\n",
            "[ 2/30] train_loss: 30.77541 valid_loss: 41.90262\n",
            "[ 3/30] train_loss: 30.49164 valid_loss: 42.27242\n",
            "[ 4/30] train_loss: 30.21376 valid_loss: 42.64670\n",
            "[ 5/30] train_loss: 29.94529 valid_loss: 43.02333\n",
            "[ 6/30] train_loss: 29.68689 valid_loss: 43.40171\n",
            "[ 7/30] train_loss: 29.43833 valid_loss: 43.78184\n",
            "[ 8/30] train_loss: 29.19962 valid_loss: 44.16203\n",
            "[ 9/30] train_loss: 28.97081 valid_loss: 44.54335\n",
            "[10/30] train_loss: 28.75091 valid_loss: 44.92569\n",
            "[11/30] train_loss: 28.54150 valid_loss: 45.30229\n",
            "[12/30] train_loss: 28.34190 valid_loss: 45.68023\n",
            "[13/30] train_loss: 28.14944 valid_loss: 46.06604\n",
            "[14/30] train_loss: 27.96009 valid_loss: 46.45539\n",
            "[15/30] train_loss: 27.77950 valid_loss: 46.84304\n",
            "[16/30] train_loss: 27.60713 valid_loss: 47.22909\n",
            "[17/30] train_loss: 27.44318 valid_loss: 47.61265\n",
            "[18/30] train_loss: 27.28679 valid_loss: 47.99519\n",
            "[19/30] train_loss: 27.13761 valid_loss: 48.37505\n",
            "[20/30] train_loss: 26.99629 valid_loss: 48.75108\n",
            "[21/30] train_loss: 26.86173 valid_loss: 49.12592\n",
            "[22/30] train_loss: 26.73345 valid_loss: 49.49876\n",
            "[23/30] train_loss: 26.61142 valid_loss: 49.86908\n",
            "[24/30] train_loss: 26.49548 valid_loss: 50.23659\n",
            "[25/30] train_loss: 26.38548 valid_loss: 50.60081\n",
            "[26/30] train_loss: 26.28128 valid_loss: 50.96116\n",
            "[27/30] train_loss: 26.18275 valid_loss: 51.31721\n",
            "[28/30] train_loss: 26.08972 valid_loss: 51.66861\n",
            "[29/30] train_loss: 26.00198 valid_loss: 52.01516\n",
            "[30/30] train_loss: 25.91931 valid_loss: 52.35644\n",
            "<bound method Module.modules of Net5(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (1): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "    (3): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (4): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")>\n",
            "[ 1/30] train_loss: 31.70317 valid_loss: 40.71295\n",
            "[ 2/30] train_loss: 31.51918 valid_loss: 40.97163\n",
            "[ 3/30] train_loss: 31.27001 valid_loss: 41.29155\n",
            "[ 4/30] train_loss: 30.96847 valid_loss: 41.69359\n",
            "[ 5/30] train_loss: 30.62249 valid_loss: 42.15405\n",
            "[ 6/30] train_loss: 30.25028 valid_loss: 42.68283\n",
            "[ 7/30] train_loss: 29.84976 valid_loss: 43.28549\n",
            "[ 8/30] train_loss: 29.42275 valid_loss: 43.97266\n",
            "[ 9/30] train_loss: 28.97766 valid_loss: 44.73848\n",
            "[10/30] train_loss: 28.51929 valid_loss: 45.60263\n",
            "[11/30] train_loss: 28.06497 valid_loss: 46.51977\n",
            "[12/30] train_loss: 27.63135 valid_loss: 47.48295\n",
            "[13/30] train_loss: 27.22607 valid_loss: 48.47721\n",
            "[14/30] train_loss: 26.85553 valid_loss: 49.49050\n",
            "[15/30] train_loss: 26.51711 valid_loss: 50.53478\n",
            "[16/30] train_loss: 26.21871 valid_loss: 51.54775\n",
            "[17/30] train_loss: 25.96487 valid_loss: 52.52349\n",
            "[18/30] train_loss: 25.75198 valid_loss: 53.45036\n",
            "[19/30] train_loss: 25.57451 valid_loss: 54.32736\n",
            "[20/30] train_loss: 25.42635 valid_loss: 55.15611\n",
            "[21/30] train_loss: 25.30628 valid_loss: 55.91329\n",
            "[22/30] train_loss: 25.21042 valid_loss: 56.60018\n",
            "[23/30] train_loss: 25.13434 valid_loss: 57.21926\n",
            "[24/30] train_loss: 25.07425 valid_loss: 57.77435\n",
            "[25/30] train_loss: 25.02703 valid_loss: 58.26835\n",
            "[26/30] train_loss: 24.98990 valid_loss: 58.70371\n",
            "[27/30] train_loss: 24.96096 valid_loss: 59.08836\n",
            "[28/30] train_loss: 24.93821 valid_loss: 59.42427\n",
            "[29/30] train_loss: 24.92046 valid_loss: 59.71899\n",
            "[30/30] train_loss: 24.90654 valid_loss: 59.97555\n",
            "<bound method Module.modules of Net6(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (1): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (2): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")>\n",
            "[ 1/30] train_loss: 30.50217 valid_loss: 42.89300\n",
            "[ 2/30] train_loss: 29.51004 valid_loss: 44.55363\n",
            "[ 3/30] train_loss: 28.53721 valid_loss: 46.27343\n",
            "[ 4/30] train_loss: 27.70044 valid_loss: 47.97488\n",
            "[ 5/30] train_loss: 27.02913 valid_loss: 49.58730\n",
            "[ 6/30] train_loss: 26.47999 valid_loss: 51.15067\n",
            "[ 7/30] train_loss: 26.05208 valid_loss: 52.59160\n",
            "[ 8/30] train_loss: 25.72745 valid_loss: 53.88558\n",
            "[ 9/30] train_loss: 25.48536 valid_loss: 55.03368\n",
            "[10/30] train_loss: 25.30655 valid_loss: 56.03449\n",
            "[11/30] train_loss: 25.17548 valid_loss: 56.89352\n",
            "[12/30] train_loss: 25.07921 valid_loss: 57.62056\n",
            "[13/30] train_loss: 25.00726 valid_loss: 58.22718\n",
            "[14/30] train_loss: 24.95128 valid_loss: 58.72523\n",
            "[15/30] train_loss: 24.90453 valid_loss: 59.12483\n",
            "[16/30] train_loss: 24.86082 valid_loss: 59.43387\n",
            "[17/30] train_loss: 24.81601 valid_loss: 59.66192\n",
            "[18/30] train_loss: 24.76688 valid_loss: 59.81566\n",
            "[19/30] train_loss: 24.70939 valid_loss: 59.89997\n",
            "[20/30] train_loss: 24.63985 valid_loss: 59.92092\n",
            "[21/30] train_loss: 24.55604 valid_loss: 59.88920\n",
            "[22/30] train_loss: 24.45564 valid_loss: 59.81158\n",
            "[23/30] train_loss: 24.33709 valid_loss: 59.69394\n",
            "[24/30] train_loss: 24.19818 valid_loss: 59.53739\n",
            "[25/30] train_loss: 24.03793 valid_loss: 59.34275\n",
            "[26/30] train_loss: 23.85359 valid_loss: 59.06767\n",
            "[27/30] train_loss: 23.64236 valid_loss: 58.73937\n",
            "[28/30] train_loss: 23.40777 valid_loss: 58.41567\n",
            "[29/30] train_loss: 23.14701 valid_loss: 58.02016\n",
            "[30/30] train_loss: 22.85045 valid_loss: 57.56490\n",
            "<bound method Module.modules of Net7(\n",
            "  (layer1): Sequential(\n",
            "    (0): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (3): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (4): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")>\n",
            "[ 1/30] train_loss: 31.74229 valid_loss: 40.64495\n",
            "[ 2/30] train_loss: 31.62905 valid_loss: 40.78475\n",
            "[ 3/30] train_loss: 31.50136 valid_loss: 40.93089\n",
            "[ 4/30] train_loss: 31.37197 valid_loss: 41.07936\n",
            "[ 5/30] train_loss: 31.24227 valid_loss: 41.23160\n",
            "[ 6/30] train_loss: 31.11201 valid_loss: 41.38709\n",
            "[ 7/30] train_loss: 30.98160 valid_loss: 41.54565\n",
            "[ 8/30] train_loss: 30.85121 valid_loss: 41.70718\n",
            "[ 9/30] train_loss: 30.72097 valid_loss: 41.87168\n",
            "[10/30] train_loss: 30.59096 valid_loss: 42.03916\n",
            "[11/30] train_loss: 30.46116 valid_loss: 42.20989\n",
            "[12/30] train_loss: 30.33123 valid_loss: 42.38474\n",
            "[13/30] train_loss: 30.20069 valid_loss: 42.56394\n",
            "[14/30] train_loss: 30.06985 valid_loss: 42.74765\n",
            "[15/30] train_loss: 29.93835 valid_loss: 42.93621\n",
            "[16/30] train_loss: 29.80641 valid_loss: 43.12953\n",
            "[17/30] train_loss: 29.67419 valid_loss: 43.32737\n",
            "[18/30] train_loss: 29.54194 valid_loss: 43.53017\n",
            "[19/30] train_loss: 29.40933 valid_loss: 43.73819\n",
            "[20/30] train_loss: 29.27667 valid_loss: 43.95105\n",
            "[21/30] train_loss: 29.14426 valid_loss: 44.16850\n",
            "[22/30] train_loss: 29.01238 valid_loss: 44.39034\n",
            "[23/30] train_loss: 28.88123 valid_loss: 44.61639\n",
            "[24/30] train_loss: 28.75102 valid_loss: 44.84647\n",
            "[25/30] train_loss: 28.62193 valid_loss: 45.08039\n",
            "[26/30] train_loss: 28.49414 valid_loss: 45.31802\n",
            "[27/30] train_loss: 28.36776 valid_loss: 45.55941\n",
            "[28/30] train_loss: 28.24270 valid_loss: 45.80489\n",
            "[29/30] train_loss: 28.11901 valid_loss: 46.05431\n",
            "[30/30] train_loss: 27.99690 valid_loss: 46.30719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "4Wv-gm6e8odH",
        "outputId": "872d9fac-a429-4949-ccd3-7bd633fd3d23"
      },
      "source": [
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xlim(0, len(train_loss)+1)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b9ZV-O587If",
        "outputId": "074d0f52-5409-4cd5-b55a-a4faa3c15f8d"
      },
      "source": [
        "i = 0\n",
        "for batch, (x, y) in enumerate(test_loader, 1):\n",
        "  i = i + 1\n",
        "  print(x[0].numpy())\n",
        "  print(y.numpy())\n",
        "  print(model(x))\n",
        "  if i > 10:\n",
        "    break\n",
        "  print()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-8.01]\n",
            " [-6.62]\n",
            " [-4.85]]\n",
            "[-2.24]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[-6.62]\n",
            " [-4.85]\n",
            " [-2.24]]\n",
            "[0.29]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[-4.85]\n",
            " [-2.24]\n",
            " [ 0.29]]\n",
            "[0.08]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[-2.24]\n",
            " [ 0.29]\n",
            " [ 0.08]]\n",
            "[1.]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[0.29]\n",
            " [0.08]\n",
            " [1.  ]]\n",
            "[1.1]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[0.08]\n",
            " [1.  ]\n",
            " [1.1 ]]\n",
            "[1.31]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[1.  ]\n",
            " [1.1 ]\n",
            " [1.31]]\n",
            "[1.24]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[1.1 ]\n",
            " [1.31]\n",
            " [1.24]]\n",
            "[1.06]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[1.31]\n",
            " [1.24]\n",
            " [1.06]]\n",
            "[1.66]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[1.24]\n",
            " [1.06]\n",
            " [1.66]]\n",
            "[1.52]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n",
            "\n",
            "[[1.06]\n",
            " [1.66]\n",
            " [1.52]]\n",
            "[1.49]\n",
            "tensor([-0.7993], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czARonOgDmbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060f0bcb-f85f-493e-a611-e52c36ae51a9"
      },
      "source": [
        "\n",
        "\n",
        "for x in results:\n",
        "  print(x,':',results[x],'\\n')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Net1(\n",
            "  (layer1): Sequential(\n",
            "    (0): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")MSELoss:  : 4.961216407561811 \n",
            "\n",
            "Test Net2(\n",
            "  (layer1): Sequential(\n",
            "    (0): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (2): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")MSELoss:  : 25.745405710245404 \n",
            "\n",
            "Test Net3(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (1): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")MSELoss:  : 4.066821258667964 \n",
            "\n",
            "Test Net4(\n",
            "  (layer1): Sequential(\n",
            "    (0): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=3, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")MSELoss:  : 32.794191431457676 \n",
            "\n",
            "Test Net5(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (1): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "    (3): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (4): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")MSELoss:  : 40.40863464915894 \n",
            "\n",
            "Test Net6(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (1): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (2): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")MSELoss:  : 40.187685639684695 \n",
            "\n",
            "Test Net7(\n",
            "  (layer1): Sequential(\n",
            "    (0): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "    (3): SelfAttention(\n",
            "      (tokeys): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (toqueries): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (tovalues): Linear(in_features=1, out_features=2, bias=False)\n",
            "      (unifyheads): Linear(in_features=2, out_features=1, bias=True)\n",
            "    )\n",
            "    (4): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")MSELoss:  : 26.749354785953116 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}